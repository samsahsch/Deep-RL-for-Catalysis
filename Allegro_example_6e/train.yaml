run: [train, test]

cutoff_radius: 6.0
chemical_symbols: [O, H] 
model_type_names: ${chemical_symbols}
monitored_metric: val0_epoch/weighted_sum
num_scalar_features: 128
data:
   _target_: nequip.data.datamodule.ASEDataModule
   seed: 456             # dataset seed for reproducibility
   split_dataset:
      file_path: all.xyz
      train: 0.8
      val: 0.1
      test: 0.1
   transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
   train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 4
    num_workers:  1
    shuffle: true
   val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 4
    num_workers: ${data.train_dataloader.num_workers}
   test_dataloader: ${data.val_dataloader}
   stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  max_epochs: 1000
  accelerator: gpu
  enable_checkpointing: true
  check_val_every_n_epoch: 1
  log_every_n_steps: 10
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}
      save_last: true

training_module:
  _target_: nequip.train.EMALightningModule
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 1.0
      forces: 1.0
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      total_energy_rmse: 1.0
      per_atom_energy_rmse: null
      forces_rmse: 1.0
      total_energy_mae: null
      per_atom_energy_mae: null
      forces_mae: null
            #per_atom_energy_mae: 1.0
            #forces_mae: 1.0
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  lr_scheduler:
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 25
      min_lr: 1e-4
      threshold: 0.02
      threshold_mode: rel
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1
  model:
    _target_: allegro.model.AllegroModel

    # If you have PyTorch >= 2.6.0 installed, and are training on GPUs, the following line uses torch.compile to speed up training
    # for more details, see https://nequip.readthedocs.io/en/latest/guide/accelerations/pt2_compilation.html
    compile_mode: compile
    # ^ if you're using PyTorch <= 2.6.0, an error will be thrown -- comment out the line to avoid it

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 2
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 32

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 2
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting `nonlinearity` to `null` means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: true

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is `true`, which is expected to be more expressive than `false`
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting `nonlinearity` to `null` means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # per-type per-atom scales and shifts
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    # ^ this should typically be the isolated atom energies for your dataset
    #   provided as a dict, e.g.
    # per_type_energy_shifts: 
    #   C: 1.234
    #   H: 2.345
    #   O: 3.456
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # ZBL pair potential (optional, can be removed or included depending on aplication)
    # see NequIP docs for details:
    # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

# global options
global_options:
  allow_tf32: True
